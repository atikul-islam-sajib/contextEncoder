path:
  RAW_DATA_PATH: "./data/raw/"
  PROCESSED_DATA_PATH: "./data/processed/"
  ARTIFACTS_PATH: "./research/artifacts/"
  TRAIN_MODELS_PATH: "./checkpoints/train_models/"
  BEST_MODEL_PATH: "./checkpoints/best_model/"
  METRCIS_PATH: "./outputs/metrics/"
  SAVE_IMAGE_PATH: "./outputs/train_images/"

dataloader:
  image_path: "./data/raw/dataset.zip"
  image_size: 128            # Pape suggested to use 128, try to use different like 256, 512, 1024 as well
  channels: 3
  batch_size: 4
  split_size: 0.30

trainer:
  name: "trainer"
  adam: True
  SGD: False
  lr: 0.0002
  beta1: 0.5                   # Used in the Adam optimizer
  beta2: 0.999                 # Used in the Adam optimizer
  momentum: 0.9                # Used for SGD optimizer
  weight_decay: 0.0001         # Used for Adam optimizer
  epochs: 100
  adversarial_lambda: 0.001    # Lmbda value added in the netG
  pixelwise_lambda: 0.999      # Lmbda value added in the netG
  steps: 10
  step_size: 10                # Used for learning rate scheduler
  gamma: 0.5                   # Used for learning rate scheduler
  device: mps                 # Can be used "cpu", "mps" as well
  lr_scheduler: False          # Used StepLR
  l1_regularization: False
  l2_regularization: False
  MLFlow: True                 # To visualize the training process in local - "mlflow ui"
  display: True                # Show the progress in each epoch
  is_weight_init: True

tester:
  model: None
  dataloader: None             # Can be "train", "valid" as well
  device: cuda                 # Can be used "mps" or "cuda" as well